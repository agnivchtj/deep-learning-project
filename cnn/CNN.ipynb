{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "We can create a Convolutional Neural Network to classify dogs and cats using pet images. CNNs are popular for working on image data, and is currently state-of-art for detecting what is contained in an image. The basic CNN structure consists of layers such as Convolution, Pooling and a Fully Connected Layer.\n",
    "\n",
    "First, we need a dataset and for this we can make use of the [Dogs vs Cats dataset provided by Microsoft](https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765). Within the directory, we have images of cats and dogs and we are looking to convert this dataset into the training set for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "directory = './pet_images'\n",
    "categories = ['Dog', 'Cat']\n",
    "\n",
    "training_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    for category in categories:\n",
    "        path = os.path.join(directory, category)\n",
    "        class_index = categories.index(category)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                new_arr = cv2.resize(img_arr, (50, 50))\n",
    "                training_data.append([new_arr, class_index])\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "create_training_data()\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to ensure the data used for training the model is balanced and there is an equal number of examples for each class (dogs and cats). If this is not done, the model may learn to predict only the class that is the most common and often get stuck there. Fortunately, this Kaggle dataset is already balanced; if it wasn't, we could think about either assigning class weights or trimming the larger set to match size of the smaller set.\n",
    "\n",
    "Then, it's important to shuffle the data and ensure the samples are not in any particular order. Otherwise, the model will go through all the dog images and learn to always predict dogs before going through the cat images and just predicting cats. Thus, the samples should interspersed amongst each other in order to eliminate bias in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for img, label in training_data:\n",
    "    x_train.append(img)\n",
    "    y_train.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Convolution` is taking the initial dataset and building feature maps from that. `Pooling` then involves selecting a region and, for example with max-pooling, taking the maximum value of that region, which then becomes the new value for the entire region. `Fully Connected Layers` are typical neural network type of layers (similar to a multilayer perceptron), where all nodes are interconnected. Each `Convolution` and `Pooling` step is a hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "702/702 [==============================] - 39s 55ms/step - loss: 0.6282 - accuracy: 0.6465 - val_loss: 0.6086 - val_accuracy: 0.6645\n",
      "Epoch 2/5\n",
      "702/702 [==============================] - 42s 60ms/step - loss: 0.5519 - accuracy: 0.7246 - val_loss: 0.5166 - val_accuracy: 0.7655\n",
      "Epoch 3/5\n",
      "702/702 [==============================] - 49s 70ms/step - loss: 0.5180 - accuracy: 0.7475 - val_loss: 0.5033 - val_accuracy: 0.7651\n",
      "Epoch 4/5\n",
      "702/702 [==============================] - 44s 62ms/step - loss: 0.5008 - accuracy: 0.7604 - val_loss: 0.4894 - val_accuracy: 0.7723\n",
      "Epoch 5/5\n",
      "702/702 [==============================] - 42s 60ms/step - loss: 0.4844 - accuracy: 0.7713 - val_loss: 0.4961 - val_accuracy: 0.7655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2882287f040>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "x_train = np.array(x_train).reshape(-1, 50, 50, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3), input_shape = x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer = 'adam', \n",
    "    loss = 'binary_crossentropy', \n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(x_train, y_train, batch_size = 32, validation_split = 0.1, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard to analyze the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorBoard is a useful application (which comes with TensorFlow) that allows us to view aspects of our model(s) in browser. We do this via a Keras callback (called `tf.keras.callbacks.TensorBoard`) which enables visualizations for TensorBoard, logging events such as summary plot metrics, training graphs, weight histograms and sampled profiles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "702/702 [==============================] - 45s 63ms/step - loss: 0.4730 - accuracy: 0.7776 - val_loss: 0.5156 - val_accuracy: 0.7535\n",
      "Epoch 2/5\n",
      "702/702 [==============================] - 44s 63ms/step - loss: 0.4597 - accuracy: 0.7830 - val_loss: 0.5113 - val_accuracy: 0.7491\n",
      "Epoch 3/5\n",
      "702/702 [==============================] - 42s 60ms/step - loss: 0.4466 - accuracy: 0.7892 - val_loss: 0.4892 - val_accuracy: 0.7671\n",
      "Epoch 4/5\n",
      "702/702 [==============================] - 42s 59ms/step - loss: 0.4359 - accuracy: 0.8000 - val_loss: 0.4699 - val_accuracy: 0.7892\n",
      "Epoch 5/5\n",
      "702/702 [==============================] - 42s 60ms/step - loss: 0.4259 - accuracy: 0.8051 - val_loss: 0.4716 - val_accuracy: 0.7828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x288241fd220>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "model_name = 'pet-cnn-64x2-{}'.format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir = 'logs/{}'.format(model_name))\n",
    "\n",
    "model.fit(x_train, y_train, batch_size = 32, validation_split = 0.1, epochs = 5, callbacks = [tensorboard])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
